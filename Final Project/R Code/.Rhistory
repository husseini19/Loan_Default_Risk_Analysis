library(ISLR)
m1 <- lm1(mpg ~ horsepower, data=Auto)
m1 <- lm(mpg ~ horsepower, data=Auto)
m1 <- lm(mpg ~ horsepower, data=Auto)
plot(m1)
plot(, which = 1)
plot(m1, which = 1)
## Plotting different charts to check if the model is valid
## Linearity and constant variable
sq.hp = auto$horsepower ^ 2
## Plotting different charts to check if the model is valid
## Linearity and constant variable
hp2 = auto$horsepower ^ 2
## Plotting different charts to check if the model is valid
## Linearity and constant variable
hp2 = Auto$horsepower ^ 2
## Plotting different charts to check if the model is valid
## Linearity and constant variable
Auto.hp2 = Auto$horsepower ^ 2
## Plotting different charts to check if the model is valid
## Linearity and constant variable
Auto$hp2 = Auto$horsepower ^ 2
m2 <- lm(mpg ~ horsepower + hp2, data=Auto)
plot(m2, which = 1)
plot(fitted(m2), residuals(m2))
#Another way to create the model
m3 <- lm(mpg ~ horsepower + polynomial(horsepower, 2), data=Auto)
#Another way to create the model
m3 <- lm(mpg ~ horsepower + poly(horsepower, 2), data=Auto)
plot(m3, which=1)
plot(m1, which=1)
Auto[334,]
lm.fit <- lm(medv ~ lstat + age, data=Boston)
library(MASS)
lm.fit <- lm(medv ~ lstat + age, data=Boston)
plot(lm.fit, which = 3)
hatvalues(lm.fit)
a <- hatvalues(lm.fit)
head(a)
a <- as.data.frame(hatvalues(lm.fit))
head(a)
subset(a, a[,1] > 3 * mean(a[,1]))
summary(lm.fit)
lm.fit.full = lm(medv ~ ., data=Boston)
summary(lm.fit.full)
b <- lm(age ~ . - medv, data=Boston)
summary(b)
res.b <- summary(b)$r.sq
vif.age <- 1 / (1 - res.b)
vif.age
## Any value that's more than 5 has a multicollinearity
install.packages("car")
library(car)
vif(lm.fit.full)
(vif(lm.fit.full))
(vif(lm.fit.full))
vif12 = vif(lm.fit.full)
install.packages("rattle")
library(rattle)
rattle()
install.packages("RGtk2")
library(rattle)
rattle()
rattle()
library(rattle)
rattle()
data <- read.csv("C:\\Users\\Kareem\\Documents\\HC Kaggle Competition Tables\application_train.csv")
data <- read.csv("C:\\Users\\Kareem\\Documents\\HC Kaggle Competition Tables\\application_train.csv")
head(data)
summary(data)
data$target <- as.factor(data$target)
data$target <- as.factor(data$TARGET)
summary(data)
data$TARGET <- as.factor(data$TARGET)
summary(data)
COR(data)
cor(data)
corr(data)
corr(data[,])
corr(data[2,12])
data[2,12]
data[2,-12]
data[0,]
data[0,12]
corr(data[0,])
cor(data[0,])
cor(data[0,-12])
cor(data[1,-12])
cor(data[1,12])
head(data)
glm.fit <- glm(TARGET ~ NAME_CONTRACT_TYPE + CODE_GENDER + FLAG_OWN_CAR + FLAG_OWN_REALTY + CNT_CHILDREN
+ AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY + AMT_GOODS_PRICE + NAME_INCOME_TYPE, data=data, family = binomial)
glm.fit <- glm(TARGET ~ NAME_CONTRACT_TYPE + CODE_GENDER + FLAG_OWN_CAR + FLAG_OWN_REALTY + CNT_CHILDREN + AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY + AMT_GOODS_PRICE + NAME_INCOME_TYPE, data=data, family = binomial)
glm.fit <- glm(TARGET ~ NAME_CONTRACT_TYPE + CODE_GENDER + FLAG_OWN_CAR + FLAG_OWN_REALTY + CNT_CHILDREN + AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY + AMT_GOODS_PRICE + NAME_INCOME_TYPE, data=data, family = binomial)
data <- read.csv("C:\\Users\\Kareem\\Documents\\HC Kaggle Competition Tables\\application_train.csv")
head(data)
data$TARGET <- as.factor(data$TARGET)
summary(data)
glm.fit <- glm(TARGET ~ NAME_CONTRACT_TYPE + CODE_GENDER + FLAG_OWN_CAR + FLAG_OWN_REALTY + CNT_CHILDREN + AMT_INCOME_TOTAL + AMT_CREDIT + AMT_ANNUITY + AMT_GOODS_PRICE + NAME_INCOME_TYPE, data=data, family = binomial)
summary(glm.fit)
pred.fit <- predict(glm.fit, data=data, type="response")
pred.fit
data <- read.csv("C:\\Users\\Kareem\\Documents\\Wayne State University\\Fall 2019\\DSA 6000\\Exercises\\Class Classification Exercise\\classification_trainig.csv")
data <- read.csv("C:\\Users\\Kareem\\Documents\\Wayne State University\\Fall 2019\\DSA 6000\\Exercises\\Class Classification Exercise\\classification_training.csv")
head(data)
data <- read.csv("C:\\Users\\Kareem\\Documents\\Wayne State University\\Fall 2019\\DSA 6000\\Exercises\\Class Classification Exercise\\classification_training.csv")
## Nulls
is.na(data)
data <- read.csv("C:\\Users\\Kareem\\Documents\\Wayne State University\\Fall 2019\\DSA 6000\\Exercises\\Class Classification Exercise\\classification_training.csv")
head(data)
## Nulls
is.na(data)
View(data)
data <- read.csv("C:\\Users\\Kareem\\Documents\\Wayne State University\\Fall 2019\\DSA 6000\\Exercises\\Class Classification Exercise\\classification_training.csv")
View(data)
glm.fit <- glm(X1~., data=data, family=binomial)
summary(glm.fit)
pred_tr <- predict(glm.fit, data, type="response")
cor(data)
View(glm.fit)
data <- read.csv("C:\\Users\\Kareem\\Documents\\Wayne State University\\Fall 2019\\DSA 6000\\Exercises\\Class Classification Exercise\\classification_training.csv")
head(data)
cor(data)
glm.fit <- glm(X1~., data=data, family=binomial)
View(glm.fit)
summary(glm.fit)
pred_tr <- predict(glm.fit, data, type="response")
df <- read.table(file="C:\\Users\\Kareem\\Documents\\Wayne State University\\Fall 2019\\DSA 6000\\Labs\\Week 11\\K Means Clusters\\Clustering Exercise\\clustering_exercise.text", header=F, sep="")
df <- read.table(file="C:\\Users\\Kareem\\Documents\\Wayne State University\\Fall 2019\\DSA 6000\\Labs\\Week 11\\K Means Clusters\\Clustering Exercise\\clustering_exercise.text", header=F, sep="")
df <- read.table(file="C:\\Users\\Kareem\\Documents\\Wayne State University\\Fall 2019\\DSA 6000\\Labs\\Week 11\\K Means Clusters\\Clustering Exercise\\clustering_exercise.txt", header=F, sep="")
head(df)
tail(df)
summar(df$V2)
summary(df$V2)
unique(df$V2)
##
kmx <- kmeans(df, 5)
kmx$cen
kmx$centers
library(rgl)
plot3d(df, col=kmx$cluster)
x1 <- data[V3, V18]
x1 <- data[V3, V17]
x1 <- data[, V17]
x1 <- data['V3', 'V18']
head(x1)
x1 <- df['V3', 'V18']
head(x1)
head(df)
x1 <- df['V3', 'V18']
x1
x1 <- as.data.fram(df['V3', 'V18'])
x1 <- as.data.frame(df['V3', 'V18'])
x1
x1 <- df['V3', 'V18']
x1
x1 <- df[, c('V3', 'V4')]
x1
x1 <- df[c('V3'), c('V18')]
x1
x1 <- df[c('V3'), -c('V18')]
x <- df[,3:18]
pca_x <- prcomp(df)
pca_x <- prcomp(x)
str(pca_x)
summary(pca_x)
##
kmx2 <- kmeans(x, 3)
kmx2$centers
plot3d(x, col=kmx2$cluster)
##
kmx2 <- kmeans(x, 6)
kmx2$centers
plot3d(x, col=kmx2$cluster)
##
x3 <- df[,3:6]
kmx3 <- kmeans(x3, 6)
kmx3$centers
plot3d(x, col=kmx3$cluster)
kmx3 <- kmeans(x3, 3)
kmx3$centers
plot3d(x, col=kmx3$cluster)
head(pca_x$x)
## Kmeans pca_x
km_pca_x <- kmeans(pca_x$x, 5)
pca_x_x <- pca_x$x
km_pca_x <- kmeans(pca_x_x[,c(1:3)],5)
plot3d(pca_x_, col=kma_pca_x)
plot3d(pca_x_x, col=kma_pca_x)
plot3d(pca_x_x, col=km_pca_x)
plot3d(pca_x_x[,c(1:3)], col=km_pca_x)
plot3d(pca_x_x[,c(1:3)], col=km_pca_x$cluster)
## Compare
compare <- data.fram(our_pca_3 = km_pca_x$cluster, true=df[,2])
## Compare
compare <- data.frame(our_pca_3 = km_pca_x$cluster, true=df[,2])
head(compare)
##
kmx <- kmeans(df[,3:5], 5)
kmx$centers
# In this analysis, we'll try to find the model that offers the best predictive ability
# Call necessary libraries
library(MASS)
library(pROC)
library(ROCR)
library(caret)
library(rattle)
library(class)
library(randomForest)
library(reshape2)
library(ggplot2)
library(gbm)
library(tidyverse)
# Create Normalize function, to be used for kNN
normalize <- function(x) {
normalized_x <- (x - min(x)) / (max(x) - min(x))
return(normalized_x)
}
## Data set up
# Set up work directory
setwd("C:\\Users\\Kareem\\Documents\\Wayne State University\\Fall 2019\\DSA 6000\\Final Project\\R Code")
# Uploading data
default_data = read.csv(file="app_final.csv", header = T)
# Removing ID column
default_data <- default_data[,2:23]
# Removing NULL cases
default_data <- default_data[complete.cases(default_data),]
# Changing TARGET column from numeric to factor
default_data$TARGET <- as.factor(default_data$TARGET)
# Adding a column of payment as a perc of income
default_data$PERC_PAYMENT_INC <- round(default_data$AMT_PAYMENT / default_data$AMT_INCOME_TOTAL, digits = 4)
# Adding a column of credit term
default_data$NUM_CREDIT_TERM <- round(default_data$AMT_CREDIT / default_data$AMT_PAYMENT, digits = 2)
# Creating subsets of training and test data
# We're using 70% as training and 30% as test (validation) data
train_size <- round(0.70 * nrow(default_data))
test_size <- nrow(default_data) - train_size
train <- sample(nrow(default_data), size = train_size, replace = FALSE)
train_data <- default_data[train,]
summary(train_data$TARGET)
test_data <- default_data[-train,]
summary(test_data$TARGET)
# Fixing the datasets
set.seed(123)
lda.final <- lda(TARGET ~ FLAG_CASH_LOAN + CODE_FEMALE + FLAG_OWN_CAR +
AMT_INCOME_TOTAL + AMT_CREDIT + AMT_PAYMENT + AMT_ITEM_PRICE +
FLAG_EDU_HIGHER + FLAG_EDU_SEC + FLAG_MS_SEP + FLAG_MS_CM +
FLAG_MS_SINGLE + DAYS_AGE + FLAG_RETIRED + DAYS_EMPLOYED +
CNT_PREV_APPS + DAYS_PREV_DPD + PERC_PAYMENT_INC + NUM_CREDIT_TERM, data=train_data)
lda.pred <- predict(lda.final, test_data)
lda.class <- lda.pred$class
table(lda.class, test_data$TARGET)
mean(lda.class == test_data$TARGET)
confusionMatrix(lda.class, test_data$TARGET)
df <- data.frame(score = lda.pred$posterior[,2], true.class = test_data$TARGET)
roc_step <- roc(test_data$TARGET, lda.pred$posterior[,2])
roc_step$auc
plot(roc(test_data$TARGET, lda.pred$posterior[,2]),legacy.axes = TRUE
, xlab="Specificity", main=paste0("LDA Model ROC curve \n"," AUC = ", round(roc_step$auc, digits = 4)), col="red")
confusionMatrix(lda.class, test_data$TARGET)
qda.final <- qda(TARGET ~ FLAG_CASH_LOAN + CODE_FEMALE + FLAG_OWN_CAR +
AMT_INCOME_TOTAL + AMT_CREDIT + AMT_PAYMENT + AMT_ITEM_PRICE +
FLAG_EDU_HIGHER + FLAG_EDU_SEC + FLAG_MS_SEP + FLAG_MS_CM +
FLAG_MS_SINGLE + DAYS_AGE + FLAG_RETIRED + DAYS_EMPLOYED +
CNT_PREV_APPS + DAYS_PREV_DPD + PERC_PAYMENT_INC + NUM_CREDIT_TERM, data=train_data)
qda.pred <- predict(qda.final, test_data)
qda.class <- qda.pred$class
table(qda.class, test_data$TARGET)
mean(qda.class == test_data$TARGET)
hist(qda.pred$posterior)
confusionMatrix(qda.class, test_data$TARGET)
df <- data.frame(score = qda.pred$posterior[,2], true.class = test_data$TARGET)
roc_step <- roc(test_data$TARGET, qda.pred$posterior[,2])
roc_step$auc
plot(roc(test_data$TARGET, qda.pred$posterior[,2]),legacy.axes = TRUE
, xlab="Specificity", main=paste0("QDA Model ROC curve \n"," AUC = ", round(roc_step$auc, digits = 4)), col="red")
confusionMatrix(qda.class, test_data$TARGET)
train.X<- as.data.frame(lapply(train_data[, c('FLAG_CASH_LOAN', 'CODE_FEMALE', 'FLAG_OWN_CAR',
'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_PAYMENT', 'AMT_ITEM_PRICE',
'FLAG_EDU_HIGHER', 'FLAG_EDU_SEC', 'FLAG_MS_SEP', 'FLAG_MS_CM',
'FLAG_MS_SINGLE', 'DAYS_AGE', 'FLAG_RETIRED', 'DAYS_EMPLOYED',
'CNT_PREV_APPS', 'DAYS_PREV_DPD', 'PERC_PAYMENT_INC', 'NUM_CREDIT_TERM')], normalize))
test.X <- as.data.frame(lapply(test_data[, c('FLAG_CASH_LOAN', 'CODE_FEMALE', 'FLAG_OWN_CAR',
'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_PAYMENT', 'AMT_ITEM_PRICE',
'FLAG_EDU_HIGHER', 'FLAG_EDU_SEC', 'FLAG_MS_SEP', 'FLAG_MS_CM',
'FLAG_MS_SINGLE', 'DAYS_AGE', 'FLAG_RETIRED', 'DAYS_EMPLOYED',
'CNT_PREV_APPS', 'DAYS_PREV_DPD', 'PERC_PAYMENT_INC', 'NUM_CREDIT_TERM')], normalize))
train.TARGET = train_data$TARGET
set.seed(1)
## Creating the KNN predic
knn.pred <- knn(train.X, test.X, train.TARGET, k=3)
confusionMatrix(knn.pred, test_data$TARGET)
roc_step <- roc(test_data$TARGET, knn.pred)
roc_step$auc
plot(roc(test_data$TARGET, knn.pred),legacy.axes = TRUE
, xlab="Specificity", main=paste0("KNN Model ROC curve \n"," AUC = ", round(roc_step$auc, digits = 4)), col="red")
## Finally, the confusion matrix
knn.pred
knn.pred <- knn(train.X, test.X, train.TARGET, k=5)
## Finally, the confusion matrix
knn.pred
confusionMatrix(knn.pred, test_data$TARGET)
knn.pred <- knn(train.X, test.X, train.TARGET, k=7)
## Finally, the confusion matrix
knn.pred
confusionMatrix(knn.pred, test_data$TARGET)
mod.rf <- randomForest(TARGET ~ FLAG_CASH_LOAN + CODE_FEMALE + FLAG_OWN_CAR +
AMT_INCOME_TOTAL + AMT_CREDIT + AMT_PAYMENT + AMT_ITEM_PRICE +
FLAG_EDU_HIGHER + FLAG_EDU_SEC + FLAG_MS_SEP + FLAG_MS_CM +
FLAG_MS_SINGLE + DAYS_AGE + FLAG_RETIRED + DAYS_EMPLOYED +
CNT_PREV_APPS + DAYS_PREV_DPD + PERC_PAYMENT_INC + NUM_CREDIT_TERM, data=train_data, prob=TRUE)
plot(mod.rf, main="Random Forest Model", xlab="# Trees")
importance(mod.rf)
pred.rf <- predict(mod.rf, newdata = test_data, type = "prob")
pred.rf[,2]
rf.pred.class <- factor(ifelse(pred.rf[,2] >=0.10, 1, 0))
confusionMatrix(rf.pred.class, test_data$TARGET)
roc_rf <- roc(test_data$TARGET, pred.rf[,1])
plot(roc_rf,legacy.axes = TRUE
, xlab="Specificity", main=paste0("RF Model ROC curve \n"," AUC = ", round(roc_rf$auc, digits = 4)), col="red")
plot(mod.rf, main="Random Forest Model", xlab="# Trees")
plot(mod.rf, main="Random Forest Model")
plot(roc_rf,legacy.axes = TRUE
, xlab="Specificity", main=paste0("RF Model ROC curve \n"," AUC = ", round(roc_rf$auc, digits = 4)), col="red")
confusionMatrix(rf.pred.class, test_data$TARGET)
gbm.final <- gbm (TARGET ~ FLAG_CASH_LOAN + CODE_FEMALE + FLAG_OWN_CAR +
AMT_INCOME_TOTAL + AMT_CREDIT + AMT_PAYMENT + AMT_ITEM_PRICE +
FLAG_EDU_HIGHER + FLAG_EDU_SEC + FLAG_MS_SEP + FLAG_MS_CM +
FLAG_MS_SINGLE + DAYS_AGE + FLAG_RETIRED + DAYS_EMPLOYED +
CNT_PREV_APPS + DAYS_PREV_DPD + PERC_PAYMENT_INC + NUM_CREDIT_TERM,
distribution = "gaussian",
data=train_data,
n.trees = 1000,
interaction.depth = 4,
shrinkage = 0.01,
cv.folds = 3)
summary(gbm.final)
pred.gbm <- predict(gbm.final, newdata = test_data, n.trees = 1000, type = "response")
pred.gbm.class <- factor(ifelse(pred.gbm-1 >=0.10, 1, 0))
mean(pred.gbm-1)
confusionMatrix(pred.gbm.class, test_data$TARGET)
roc_step <- roc(test_data$TARGET, pred.gbm)
roc_step$auc
plot(roc(test_data$TARGET, pred.gbm),legacy.axes = TRUE
, xlab="Specificity", main=paste0("GBM Model ROC curve \n"," AUC = ", round(roc_step$auc, digits = 4)), col="red")
roc_step <- roc(test_data$TARGET, pred.gbm-1)
roc_step$auc
plot(roc(test_data$TARGET, pred.gbm),legacy.axes = TRUE
, xlab="Specificity", main=paste0("GBM Model ROC curve \n"," AUC = ", round(roc_step$auc, digits = 4)), col="red")
summary(gbm.final)
plot(roc(test_data$TARGET, pred.gbm),legacy.axes = TRUE
, xlab="Specificity", main=paste0("GBM Model ROC curve \n"," AUC = ", round(roc_step$auc, digits = 4)), col="red")
confusionMatrix(pred.gbm.class, test_data$TARGET)
summary(gbm.final)
pred.gbm.class <- factor(ifelse(pred.gbm-1 >=0.14, 1, 0))
mean(pred.gbm-1)
confusionMatrix(pred.gbm.class, test_data$TARGET)
importance(mod.rf)
# Model with all features
glm.fit.all <- glm(TARGET~., data=train_data, family="binomial")
summary(glm.fit.all)
# Assessing the model, running it on the test data
glm.pred.all <- predict(glm.fit.all, test_data, type="response")
# Probability distribution
hist(glm.pred.all, breaks=20)
# Using 10% threshold based on the probability distribution
glm.pred.class <- factor(ifelse(glm.pred.all >=0.10, 1, 0))
mean(glm.pred.class == test_data$TARGET)
confusionMatrix(glm.pred.class, test_data$TARGET)
# Using 10% threshold based on the probability distribution
glm.pred.class <- factor(ifelse(glm.pred.all >=0.14, 1, 0))
mean(glm.pred.class == test_data$TARGET)
confusionMatrix(glm.pred.class, test_data$TARGET)
# Using 10% threshold based on the probability distribution
glm.pred.class <- factor(ifelse(glm.pred.all >=0.12, 1, 0))
mean(glm.pred.class == test_data$TARGET)
confusionMatrix(glm.pred.class, test_data$TARGET)
# Using 10% threshold based on the probability distribution
glm.pred.class <- factor(ifelse(glm.pred.all >=0.10, 1, 0))
mean(glm.pred.class == test_data$TARGET)
confusionMatrix(glm.pred.class, test_data$TARGET)
# After selecting the best variables, creating a new model
glm.final = glm(formula = TARGET ~ FLAG_CASH_LOAN + CODE_FEMALE + FLAG_OWN_CAR +
AMT_INCOME_TOTAL + AMT_CREDIT + AMT_PAYMENT + AMT_ITEM_PRICE +
FLAG_EDU_HIGHER + FLAG_EDU_SEC + FLAG_MS_SEP + FLAG_MS_CM +
FLAG_MS_SINGLE + DAYS_AGE + FLAG_RETIRED + DAYS_EMPLOYED +
CNT_PREV_APPS + DAYS_PREV_DPD + PERC_PAYMENT_INC + NUM_CREDIT_TERM,
family = "binomial", data = train_data)
summary(glm.final)
# Predicting the test TARGET
glm.pred <- predict(glm.final, test_data, type="response")
hist(glm.pred, breaks=50)
glm.pred.class <- factor(ifelse(glm.pred >=0.10, 1, 0))
#table(glm.pred.class, test_data$TARGET)
mean(glm.pred.class == test_data$TARGET)
confusionMatrix(glm.pred.class, test_data$TARGET)
glm.pred.class <- factor(ifelse(glm.pred >=0.12, 1, 0))
#table(glm.pred.class, test_data$TARGET)
mean(glm.pred.class == test_data$TARGET)
confusionMatrix(glm.pred.class, test_data$TARGET)
glm.pred.class <- factor(ifelse(glm.pred >=0.14, 1, 0))
#table(glm.pred.class, test_data$TARGET)
mean(glm.pred.class == test_data$TARGET)
confusionMatrix(glm.pred.class, test_data$TARGET)
glm.pred.class <- factor(ifelse(glm.pred >=0.12, 1, 0))
#table(glm.pred.class, test_data$TARGET)
mean(glm.pred.class == test_data$TARGET)
confusionMatrix(glm.pred.class, test_data$TARGET)
#Area under the ROC curve
roc_step <- roc(test_data$TARGET, glm.pred)
roc_step$auc
plot(roc(test_data$TARGET, glm.pred),legacy.axes = TRUE
, xlab="Specificity", main=paste0("Log Reg ROC curve \n"," AUC = ", round(roc_step$auc, digits = 4)), col="red")
summary(glm.final)
lda.final <- lda(TARGET ~ FLAG_CASH_LOAN + CODE_FEMALE + FLAG_OWN_CAR +
AMT_INCOME_TOTAL + AMT_CREDIT + AMT_PAYMENT + AMT_ITEM_PRICE +
FLAG_EDU_HIGHER + FLAG_EDU_SEC + FLAG_MS_SEP + FLAG_MS_CM +
FLAG_MS_SINGLE + DAYS_AGE + FLAG_RETIRED + DAYS_EMPLOYED +
CNT_PREV_APPS + DAYS_PREV_DPD + PERC_PAYMENT_INC + NUM_CREDIT_TERM, data=train_data)
summary(lda.final)
hist(lda.pred$posterior)
hist(lda.pred$posterior, main='LDA Posterior Disribution')
confusionMatrix(lda.class, test_data$TARGET)
## Finally, the confusion matrix
knn.pred
rf.pred.class <- factor(ifelse(pred.rf[,2] >=0.12, 1, 0))
confusionMatrix(rf.pred.class, test_data$TARGET)
